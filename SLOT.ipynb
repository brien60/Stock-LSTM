{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a214f642",
   "metadata": {},
   "source": [
    "SLOT (Self-supervised Learning of Tweets for Capturing Multi-level Price Trends) aims to address\n",
    "1. the sparsity of tweets, with the number of tweets being heavily biased towards the most popular stocks.\n",
    "2. the fact that tweets have noisy information that are often irrelevant to the actual stock movement.\n",
    "\n",
    "The first problem was addressed by having SLOT learn the stock and tweet embeddings in the same vector space through self-supervised learning. This allows the use of any tweet for even unpopular stocks.\n",
    "\n",
    "To tackle the second problem, SLOT uses tweets to learn multi-level relationships between stocks, rather than using them as direct evidence for stock prediction (e.g. positive sentiment = up)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dccb983",
   "metadata": {},
   "source": [
    "### Attention LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97e35be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class ALSTM:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.ln = nn.Linear(hidden_size, hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.u = nn.Parameter(data=torch.randn(hidden_size))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, input_size)\n",
    "        # output: (batch, seq_len, hidden_size)\n",
    "        # h_n, c_n : (num_layers, batch, hidden_size)\n",
    "        output, h_n, c_n = self.lstm(x)\n",
    "        output = self.tanh(self.ln(output))\n",
    "\n",
    "        # query: u, key: output, values: output\n",
    "        attn_scores = torch.matmul(output, self.u) # (batch, seq_len, hidden_size) @ (hidden_size) -> (batch, seq_len)\n",
    "        weights = attn_scores / attn_scores.sum(dim=1, keepdim=True)\n",
    "        weights = weights.unsqueeze(dim=-1) # (batch, seq_len, 1)\n",
    "        \n",
    "        # (batch, seq_len, 1) * (batch, seq_len, hidden_size) -> (batch, seq_len, hidden_size)\n",
    "        # (batch, seq_len, hidden_size) -> (batch, hidden_size)\n",
    "        h_attn = (weights * output).sum(dim=1)\n",
    "\n",
    "        # (batch, hidden_size) || (batch, hidden_size) -> (batch, 2*hidden_size)\n",
    "        h_out = torch.cat((h_n[0], h_attn), dim=1) # both the general summary and the attention\n",
    "\n",
    "        return h_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8313c695",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SLOT:\n",
    "    def __init__(self, input_size, hidden_size, output_size = 1):\n",
    "        self.ln_1 = nn.Linear(3*input_size, 3*input_size)\n",
    "        self.alstm = ALSTM(input_size=3*input_size, hidden_size=hidden_size)\n",
    "        self.ln_f = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "    def forward(self, features, global_trend, local_trend):\n",
    "        # features, global_trend, local_trend: (batch, seq_len, input_size)\n",
    "        final_input = torch.cat((features, global_trend, local_trend), dim=-1) # (batch, 3*input_size)\n",
    "        finall_input = self.ln_1(final_input)\n",
    "        h_out = self.alstm(final_input) # (batch, 2*hidden_size)\n",
    "        y_pred = self.ln_f(h_out) # (batch, output_size)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00548375",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock-lstm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
