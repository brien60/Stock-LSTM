{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a214f642",
   "metadata": {},
   "source": [
    "SLOT (Self-supervised Learning of Tweets for Capturing Multi-level Price Trends) aims to address\n",
    "1. the sparsity of tweets, with the number of tweets being heavily biased towards the most popular stocks.\n",
    "2. the fact that tweets have noisy information that are often irrelevant to the actual stock movement.\n",
    "\n",
    "The first problem was addressed by having SLOT learn the stock and tweet embeddings in the same vector space through self-supervised learning. This allows the use of any tweet for even unpopular stocks.\n",
    "\n",
    "To tackle the second problem, SLOT uses tweets to learn multi-level relationships between stocks, rather than using them as direct evidence for stock prediction (e.g. positive sentiment = up)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dccb983",
   "metadata": {},
   "source": [
    "## Attention LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97e35be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class ALSTM:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.ln = nn.Linear(hidden_size, hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.u = nn.Parameter(data=torch.randn(hidden_size))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, input_size)\n",
    "        # output: (batch, seq_len, hidden_size)\n",
    "        # h_n, c_n : (num_layers, batch, hidden_size)\n",
    "        output, h_n, c_n = self.lstm(x)\n",
    "        output = self.tanh(self.ln(output))\n",
    "\n",
    "        # query: u, key: output, values: output\n",
    "        attn_scores = torch.matmul(output, self.u) # (batch, seq_len, hidden_size) @ (hidden_size) -> (batch, seq_len)\n",
    "        weights = attn_scores / attn_scores.sum(dim=1, keepdim=True)\n",
    "        weights = weights.unsqueeze(dim=-1) # (batch, seq_len, 1)\n",
    "        \n",
    "        # (batch, seq_len, 1) * (batch, seq_len, hidden_size) -> (batch, seq_len, hidden_size)\n",
    "        # (batch, seq_len, hidden_size) -> (batch, hidden_size)\n",
    "        h_attn = (weights * output).sum(dim=1)\n",
    "\n",
    "        # (batch, hidden_size) || (batch, hidden_size) -> (batch, 2*hidden_size)\n",
    "        h_out = torch.cat((h_n[0], h_attn), dim=1) # both the general summary and the attention\n",
    "\n",
    "        return h_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8313c695",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SLOT:\n",
    "    def __init__(self, input_size, hidden_size, output_size = 1):\n",
    "        self.ln_1 = nn.Linear(3*input_size, 3*input_size)\n",
    "        self.alstm = ALSTM(input_size=3*input_size, hidden_size=hidden_size)\n",
    "        self.ln_f = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "    def forward(self, features, global_trend, local_trend):\n",
    "        # features, global_trend, local_trend: (batch, seq_len, input_size)\n",
    "        final_input = torch.cat((features, global_trend, local_trend), dim=-1) # (batch, 3*input_size)\n",
    "        finall_input = self.ln_1(final_input)\n",
    "        h_out = self.alstm(final_input) # (batch, 2*hidden_size)\n",
    "        y_pred = self.ln_f(h_out) # (batch, output_size)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00548375",
   "metadata": {},
   "source": [
    "## Self-supervised Learning of Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538d37b0",
   "metadata": {},
   "source": [
    "The goal is to learn tweet (h_e) and stock (h_s) embeddings in the same semantic space, that is, learning the embeddings together such that one embedding can be used to query for the other. \n",
    "- a stock embedding and the embedding of tweet relevant to it are close together (higher dot product)\n",
    "- solves problem of tweet sparsity; the model can associate stocks with tweets that don't directly mention it as long as they are close in vector space.\n",
    "\n",
    "This was done by training for stock identification: predict the mentioned stock in a tweet when the stock symbol is masked.\n",
    "\n",
    "First, every tweet is tokenized with Sentence Piece. Next, the tokens that correspond to the stock the tweet mentions are masked with the special token MASK.\n",
    "- \"Thank you Apple for the new iPhone.\" -> \"Thank you [MASK] for the new iPhone.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d17860",
   "metadata": {},
   "source": [
    "### Stock Identification Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2a2c7c",
   "metadata": {},
   "source": [
    "Use a BiLSTM (need to understand the context on both sides of the mask token).\n",
    "- Using a transformer risks overfitting.\n",
    "\n",
    "The stock embedding is a learnable parameter. \n",
    "\n",
    "The hidden state vector generated at the masked token is used as the tweet embedding because it\n",
    "- captures the immediate left (via the forward LSTM) and right context (via the backward LSTM) of the tweet, making it exactly what we need for stock identification,\n",
    "- and it is the part of the tweet that most connects to mentioned stock.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087eca54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockIdentification:\n",
    "    def __init__(self, num_stocks, embd_size, hidden_size):\n",
    "        self.embd_size = embd_size\n",
    "\n",
    "        self.token_embd = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                       embedding_dim=embd_size)\n",
    "        \n",
    "\n",
    "        self.h_s = nn.Embedding(num_embeddings=num_stocks, \n",
    "                                embedding_dim=embd_size)\n",
    "        \n",
    "        self.bi_lstm = nn.LSTM(\n",
    "            input_size=embd_size, \n",
    "            hidden_size=hidden_size,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (B, T) # tokenize before passing in\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa8437a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock-lstm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
