{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b53486d",
   "metadata": {},
   "source": [
    "### RNN Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd1dd61",
   "metadata": {},
   "source": [
    "<img src=\"images/RNN.png\" alt=\"RNN Diagram\" height =\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22546662",
   "metadata": {},
   "source": [
    "**The weights and biases in an RNN are shared across all time steps**\n",
    "\n",
    "$$h_t = \\tanh(Ux_t + Wh_{t-1} + b_h)$$\n",
    "$$o_t = Vh_t + b_o$$\n",
    "\n",
    "$W, U$ are weights for the hidden state and the input state.\n",
    "- $W$ controls how much of the last time step's hidden state we want to keep\n",
    "- $U$ controls how much of the new input we want.\n",
    "\n",
    "$V$ is the weight that maps the hidden state to the output\n",
    "\n",
    "$o_t$ is the output at time $t$\n",
    "\n",
    "$b_h$ and $b_o$ are biases.\n",
    "\n",
    "The hidden state ($h$) serves as the memory of the network.\n",
    "- $h_t$ has information from all the time steps before t, albeit in a compressed way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846e6a69",
   "metadata": {},
   "source": [
    "We can concatenate the hidden state and the input into a single long vector $[h_{t-1}, x_t]$\n",
    "\n",
    "Example:\n",
    "\n",
    "**No concatenation:**\n",
    "- $x_t: (10,)$\n",
    "- $h_{t-1}: (64,)$\n",
    "- $U: (64, 10)$\n",
    "- $W: (64, 64)$\n",
    "- $b_h: (64,)$\n",
    "\n",
    "Multiplications:\n",
    "- $Ux_t: (64,)$\n",
    "- $Wh_{t-1}: (64,)$\n",
    "- $\\tanh(Ux_t + Wh_{t-1} + b_h): (64,)$\n",
    "\n",
    "\n",
    "**With concatenation:**\n",
    "- $[h_{t-1}, x_t]: (64,) + (10,) = (74,)$\n",
    "- $W: (64, 74)$\n",
    "- $b_h: (64,)$\n",
    "\n",
    "Multiplications:\n",
    "- $W[h_{t-1}, x_t]: (64,)$\n",
    "- $\\tanh(W[h_{t-1}, x_t] + b_h): (64,)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cddc760",
   "metadata": {},
   "source": [
    "The problem with RNNs are vanishing/exploding gradients, as $h_{t-1}$ is multiplied by $W$ with each time step. This causes RNNs to forget long-term dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d19bbd9",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440976fd",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/LSTM.png\" alt=\"cell_state\" height=\"300\">\n",
    "</div>\n",
    "\n",
    "Each module in an LSTM contains four layers.\n",
    "\n",
    "The key to LSTMs is the cell state ($C$), as it is the long-term memory of the network.\n",
    "\n",
    "The cell state just travels straight down like on a conveyor belt, with the LSTM being able to remove or add information to the cell state through **gates**.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/cell_state.png\" alt=\"cell_state\" height=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051d8986",
   "metadata": {},
   "source": [
    "Gates are composed of sigmoid layer and an element-wise multiplication.\n",
    "An output of zero means let nothing through, and one means let everything through. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c53fcdf",
   "metadata": {},
   "source": [
    "Forget Gate Layer\n",
    "- The forget layer decides what information to remove from $C_{t-1}$.\n",
    "\n",
    "- It looks at $h_{t-1}$ and $x_t$ and outputs a number between 0 and 1.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/forget_layer.png\" alt=\"forget\" height=\"300\">\n",
    "</div>\n",
    "\n",
    "The input gate layer decides which parts of the cell state to update\n",
    "- 0 is don't update\n",
    "- 1 is fully update\n",
    "\n",
    "The tanh layer creates a vector of candidate values that could be added to the cell state (values between -1 and 1)\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/input_gate_and_tanh.png\" height=\"300\">\n",
    "</div>\n",
    "\n",
    "Multiply the output from the input gate and the tanh layer to get the actual update to the cell state:\n",
    "$$i_t \\odot \\tilde{C}_t$$\n",
    "\n",
    "\n",
    "The new cell state is $f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/update.png\" height=\"300\">\n",
    "</div>\n",
    "\n",
    "The output gate is another sigmoid layer and it decides what parts of the cell state to reveal as $h_t$\n",
    "\n",
    "The cell state is pushed through tanh to make the values between -1 and 1, and this is multiplied by the output gate to get $h_t$\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/output.png\" height=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5262858",
   "metadata": {},
   "source": [
    "Both $h_t$ and $C_t$ are passed to the next time step, but $h_t$ is also stored as the step's output.\n",
    "\n",
    "- $C_t$ = stable long-term memory.\n",
    "- $h_t$ = short-term working memory + external output.\n",
    "\n",
    "Thus the name Long Short-Term Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5066f429",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4294a0d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46973116",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
